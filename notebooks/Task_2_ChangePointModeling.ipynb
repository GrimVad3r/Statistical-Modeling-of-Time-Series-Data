{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Change Point Modeling and Insight Generation\n",
    "## Bayesian Analysis of Brent Oil Prices\n",
    "\n",
    "**Objective**: Apply Bayesian change point detection to identify and quantify structural breaks in Brent oil prices.\n",
    "\n",
    "**Due Date**: Final Submission - Tuesday, 10 Feb 2026, 8:00 PM UTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 1: Initialize Environment and Logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyMC imports\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "        logging.FileHandler('task_2_modeling.log')\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info('Task 2: Change Point Modeling - Analysis Started')\n",
    "logger.info(f'Timestamp: {datetime.now()}')\n",
    "logger.info(f'PyMC Version: {pm.__version__}')\n",
    "\n",
    "# Visualization settings\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preparation and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 2: Data Loading and Preparation\n",
    "class DataPreparator:\n",
    "    \"\"\"\n",
    "    Modular class for data preparation in change point analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.df = None\n",
    "    \n",
    "    def load_and_prepare(self):\n",
    "        \"\"\"\n",
    "        Load data and perform preprocessing.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f'Loading data from {self.filepath}')\n",
    "            self.df = pd.read_csv(self.filepath)\n",
    "            \n",
    "            # Convert date and price\n",
    "            self.df['Date'] = pd.to_datetime(self.df['Date'], format='%d-%b-%y')\n",
    "            self.df['Price'] = pd.to_numeric(self.df['Price'], errors='coerce')\n",
    "            \n",
    "            # Sort and reset index\n",
    "            self.df = self.df.sort_values('Date').reset_index(drop=True)\n",
    "            \n",
    "            # Remove any NaN values\n",
    "            self.df = self.df.dropna()\n",
    "            \n",
    "            self.logger.info(f'Data prepared: {len(self.df)} records from {self.df[\"Date\"].min().date()} to {self.df[\"Date\"].max().date()}')\n",
    "            return self.df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f'Error loading data: {str(e)}')\n",
    "            raise\n",
    "    \n",
    "    def prepare_for_modeling(self):\n",
    "        \"\"\"\n",
    "        Prepare data specifically for change point modeling.\n",
    "        \"\"\"\n",
    "        self.logger.info('Preparing data for Bayesian modeling')\n",
    "        \n",
    "        # Calculate returns (alternative to using prices directly)\n",
    "        self.df['Returns'] = self.df['Price'].pct_change() * 100\n",
    "        self.df['Log_Returns'] = np.log(self.df['Price'] / self.df['Price'].shift(1))\n",
    "        \n",
    "        # Fill NaN from differencing\n",
    "        self.df['Returns'] = self.df['Returns'].fillna(0)\n",
    "        self.df['Log_Returns'] = self.df['Log_Returns'].fillna(0)\n",
    "        \n",
    "        # Create time index\n",
    "        self.df['Time_Index'] = np.arange(len(self.df))\n",
    "        \n",
    "        self.logger.info('Data prepared for modeling with returns calculated')\n",
    "        return self.df\n",
    "\n",
    "# Load and prepare data\n",
    "preparer = DataPreparator('BrentOilPrices.csv')\n",
    "df = preparer.load_and_prepare()\n",
    "df = preparer.prepare_for_modeling()\n",
    "\n",
    "print(f'\\nData shape: {df.shape}')\n",
    "print(f'Date range: {df[\"Date\"].min()} to {df[\"Date\"].max()}')\n",
    "print(f'\\nFirst rows:\\n{df.head()}')\n",
    "print(f'\\nLast rows:\\n{df.tail()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 3: Exploratory Data Analysis\n",
    "class EDAVisualizer:\n",
    "    \"\"\"\n",
    "    Create exploratory visualizations of the time series.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def plot_price_and_returns(self):\n",
    "        \"\"\"\n",
    "        Plot price series and returns side by side.\n",
    "        \"\"\"\n",
    "        self.logger.info('Creating price and returns visualizations')\n",
    "        \n",
    "        fig, axes = plt.subplots(3, 1, figsize=(16, 10))\n",
    "        \n",
    "        # Price series\n",
    "        axes[0].plot(self.data['Date'], self.data['Price'], color='darkblue', linewidth=1)\n",
    "        axes[0].set_title('Brent Crude Oil Prices (1987-2022)', fontsize=12, fontweight='bold')\n",
    "        axes[0].set_ylabel('Price (USD/barrel)')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Daily returns\n",
    "        axes[1].plot(self.data['Date'], self.data['Returns'], color='darkgreen', linewidth=0.5)\n",
    "        axes[1].set_title('Daily Percentage Returns', fontsize=12, fontweight='bold')\n",
    "        axes[1].set_ylabel('Daily Return (%)')\n",
    "        axes[1].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Log returns distribution\n",
    "        axes[2].hist(self.data['Log_Returns'], bins=100, color='darkred', alpha=0.7, edgecolor='black')\n",
    "        axes[2].set_title('Distribution of Log Returns', fontsize=12, fontweight='bold')\n",
    "        axes[2].set_xlabel('Log Return')\n",
    "        axes[2].set_ylabel('Frequency')\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('task2_01_eda_visualizations.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        self.logger.info('EDA visualizations saved')\n",
    "    \n",
    "    def plot_rolling_statistics(self):\n",
    "        \"\"\"\n",
    "        Plot rolling mean and volatility.\n",
    "        \"\"\"\n",
    "        self.logger.info('Creating rolling statistics visualizations')\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 1, figsize=(16, 8))\n",
    "        \n",
    "        # Rolling mean\n",
    "        rolling_mean_30 = self.data['Price'].rolling(window=30).mean()\n",
    "        rolling_mean_90 = self.data['Price'].rolling(window=90).mean()\n",
    "        \n",
    "        axes[0].plot(self.data['Date'], self.data['Price'], label='Daily Price', linewidth=0.5, alpha=0.5)\n",
    "        axes[0].plot(self.data['Date'], rolling_mean_30, label='30-Day Moving Avg', linewidth=2)\n",
    "        axes[0].plot(self.data['Date'], rolling_mean_90, label='90-Day Moving Avg', linewidth=2)\n",
    "        axes[0].set_title('Price with Moving Averages', fontsize=12, fontweight='bold')\n",
    "        axes[0].set_ylabel('Price (USD/barrel)')\n",
    "        axes[0].legend(loc='best')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Rolling volatility\n",
    "        rolling_vol_30 = self.data['Log_Returns'].rolling(window=30).std()\n",
    "        rolling_vol_60 = self.data['Log_Returns'].rolling(window=60).std()\n",
    "        \n",
    "        axes[1].plot(self.data['Date'], rolling_vol_30, label='30-Day Rolling Vol', linewidth=1.5)\n",
    "        axes[1].plot(self.data['Date'], rolling_vol_60, label='60-Day Rolling Vol', linewidth=1.5)\n",
    "        axes[1].fill_between(self.data['Date'], rolling_vol_30, alpha=0.3)\n",
    "        axes[1].set_title('Rolling Volatility (30-day and 60-day)', fontsize=12, fontweight='bold')\n",
    "        axes[1].set_ylabel('Standard Deviation')\n",
    "        axes[1].set_xlabel('Year')\n",
    "        axes[1].legend(loc='best')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('task2_02_rolling_statistics.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        self.logger.info('Rolling statistics plot saved')\n",
    "\n",
    "# Create EDA visualizations\n",
    "eda = EDAVisualizer(df)\n",
    "eda.plot_price_and_returns()\n",
    "eda.plot_rolling_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Bayesian Change Point Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 4: Build Bayesian Change Point Model\n",
    "class BayesianChangePointModel:\n",
    "    \"\"\"\n",
    "    Build and fit a Bayesian change point model using PyMC.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, variable='Price'):\n",
    "        self.data = data.copy()\n",
    "        self.variable = variable\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.model = None\n",
    "        self.trace = None\n",
    "        self.posterior = None\n",
    "        \n",
    "        self.logger.info(f'Initializing Bayesian Change Point Model for {variable}')\n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Build the PyMC model with change point detection.\n",
    "        Model structure:\n",
    "        - Prior for change point (tau): Discrete uniform over all time indices\n",
    "        - Prior for mean before change point (mu1): Normal distribution\n",
    "        - Prior for mean after change point (mu2): Normal distribution\n",
    "        - Prior for sigma: Exponential (half-normal would also work)\n",
    "        - Likelihood: Normal distribution with switching mean\n",
    "        \"\"\"\n",
    "        self.logger.info('Building PyMC model')\n",
    "        \n",
    "        # Extract price data and standardize\n",
    "        price_data = self.data[self.variable].values\n",
    "        n = len(price_data)\n",
    "        \n",
    "        # Standardize for better sampling\n",
    "        price_mean = price_data.mean()\n",
    "        price_std = price_data.std()\n",
    "        price_standardized = (price_data - price_mean) / price_std\n",
    "        \n",
    "        self.logger.info(f'Building model with {n} observations')\n",
    "        self.logger.info(f'Price data - Mean: {price_mean:.2f}, Std: {price_std:.2f}')\n",
    "        \n",
    "        with pm.Model() as model:\n",
    "            # Priors\n",
    "            # Change point: discrete uniform over all time points\n",
    "            tau = pm.DiscreteUniform('tau', lower=1, upper=n-2)\n",
    "            \n",
    "            # Means before and after change point\n",
    "            mu1 = pm.Normal('mu1', mu=0, sigma=1)\n",
    "            mu2 = pm.Normal('mu2', mu=0, sigma=1)\n",
    "            \n",
    "            # Standard deviation (same for both regimes)\n",
    "            sigma = pm.Exponential('sigma', lam=1)\n",
    "            \n",
    "            # Switch function to select mean based on time\n",
    "            idx = np.arange(n)\n",
    "            mu = pm.math.switch(tau >= idx, mu1, mu2)\n",
    "            \n",
    "            # Likelihood\n",
    "            likelihood = pm.Normal('obs', mu=mu, sigma=sigma, observed=price_standardized)\n",
    "        \n",
    "        self.model = model\n",
    "        self.logger.info('Model structure created successfully')\n",
    "        return self.model\n",
    "    \n",
    "    def fit_model(self, draws=2000, tune=1000, cores=2, chains=2, random_seed=42):\n",
    "        \"\"\"\n",
    "        Fit the model using MCMC sampling.\n",
    "        \"\"\"\n",
    "        self.logger.info(f'Starting MCMC sampling: {draws} draws, {tune} tuning steps, {chains} chains')\n",
    "        \n",
    "        with self.model:\n",
    "            self.trace = pm.sample(\n",
    "                draws=draws,\n",
    "                tune=tune,\n",
    "                cores=cores,\n",
    "                chains=chains,\n",
    "                random_seed=random_seed,\n",
    "                return_inferencedata=True,\n",
    "                progressbar=True,\n",
    "                target_accept=0.9\n",
    "            )\n",
    "        \n",
    "        self.posterior = self.trace.posterior\n",
    "        self.logger.info('MCMC sampling completed')\n",
    "        return self.trace\n",
    "    \n",
    "    def check_convergence(self):\n",
    "        \"\"\"\n",
    "        Check model convergence using diagnostic metrics.\n",
    "        \"\"\"\n",
    "        self.logger.info('Checking convergence diagnostics')\n",
    "        \n",
    "        summary = az.summary(self.trace, var_names=['tau', 'mu1', 'mu2', 'sigma'])\n",
    "        \n",
    "        print('\\nMODEL SUMMARY - CONVERGENCE DIAGNOSTICS')\n",
    "        print('='*80)\n",
    "        print(summary)\n",
    "        print('\\nKey Metrics Interpretation:')\n",
    "        print('  • r_hat: Should be < 1.01 (close to 1.0 indicates convergence)')\n",
    "        print('  • ess_bulk: Effective sample size (higher is better)')\n",
    "        print('  • ess_tail: Effective sample size for tail (higher is better)')\n",
    "        print('='*80)\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def plot_trace(self):\n",
    "        \"\"\"\n",
    "        Plot trace plots to assess mixing and convergence.\n",
    "        \"\"\"\n",
    "        self.logger.info('Creating trace plots')\n",
    "        \n",
    "        az.plot_trace(self.trace, var_names=['tau', 'mu1', 'mu2', 'sigma'])\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('task2_03_trace_plots.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        self.logger.info('Trace plots saved')\n\n",
    "# Build and fit the model\n",
    "logger.info('='*80)\n",
    "logger.info('BUILDING BAYESIAN CHANGE POINT MODEL')\n",
    "logger.info('='*80)\n",
    "\n",
    "cp_model = BayesianChangePointModel(df, variable='Price')\n",
    "cp_model.build_model()\n",
    "\n",
    "# Note: For demonstration, we use fewer samples. Increase for production.\n",
    "cp_model.fit_model(draws=1000, tune=500, cores=2, chains=2, random_seed=42)\n",
    "\n",
    "# Check convergence\n",
    "summary = cp_model.check_convergence()\n",
    "cp_model.plot_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Model Interpretation and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 5: Extract and Interpret Results\n",
    "class ChangePointInterpreter:\n",
    "    \"\"\"\n",
    "    Extract, analyze, and interpret change point model results.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, trace, data):\n",
    "        self.model = model\n",
    "        self.trace = trace\n",
    "        self.data = data\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def extract_change_point(self):\n",
    "        \"\"\"\n",
    "        Extract change point posterior samples and statistics.\n",
    "        \"\"\"\n",
    "        self.logger.info('Extracting change point from posterior')\n",
    "        \n",
    "        tau_samples = self.trace.posterior['tau'].values.flatten()\n",
    "        \n",
    "        # Calculate statistics\n",
    "        tau_mean = np.mean(tau_samples)\n",
    "        tau_median = np.median(tau_samples)\n",
    "        tau_std = np.std(tau_samples)\n",
    "        tau_hdi = az.hdi(self.trace, var_names=['tau'])['tau'].values\n",
    "        \n",
    "        # Convert to date\n",
    "        cp_date = self.data.iloc[int(tau_median)]['Date']\n",
    "        \n",
    "        results = {\n",
    "            'tau_mean': tau_mean,\n",
    "            'tau_median': tau_median,\n",
    "            'tau_std': tau_std,\n",
    "            'tau_hdi': tau_hdi,\n",
    "            'cp_date': cp_date,\n",
    "            'tau_samples': tau_samples\n",
    "        }\n",
    "        \n",
    "        self.logger.info(f'Change point detected at index {tau_median} ({cp_date.date()})')\n",
    "        return results\n",
    "    \n",
    "    def extract_parameters(self):\n",
    "        \"\"\"\n",
    "        Extract parameter estimates before and after change point.\n",
    "        \"\"\"\n",
    "        self.logger.info('Extracting model parameters')\n",
    "        \n",
    "        mu1_samples = self.trace.posterior['mu1'].values.flatten()\n",
    "        mu2_samples = self.trace.posterior['mu2'].values.flatten()\n",
    "        sigma_samples = self.trace.posterior['sigma'].values.flatten()\n",
    "        \n",
    "        # Denormalize prices\n",
    "        price_mean = self.data['Price'].mean()\n",
    "        price_std = self.data['Price'].std()\n",
    "        \n",
    "        mu1_denorm = mu1_samples * price_std + price_mean\n",
    "        mu2_denorm = mu2_samples * price_std + price_mean\n",
    "        sigma_denorm = sigma_samples * price_std\n",
    "        \n",
    "        parameters = {\n",
    "            'mu1_mean': np.mean(mu1_denorm),\n",
    "            'mu1_hdi': az.hdi(self.trace, var_names=['mu1'])['mu1'].values * price_std + price_mean,\n",
    "            'mu2_mean': np.mean(mu2_denorm),\n",
    "            'mu2_hdi': az.hdi(self.trace, var_names=['mu2'])['mu2'].values * price_std + price_mean,\n",
    "            'sigma_mean': np.mean(sigma_denorm),\n",
    "            'mu1_samples': mu1_denorm,\n",
    "            'mu2_samples': mu2_denorm,\n",
    "            'sigma_samples': sigma_denorm\n",
    "        }\n",
    "        \n",
    "        self.logger.info(f'Mean before change point: ${parameters[\"mu1_mean\"]:.2f}')\n",
    "        self.logger.info(f'Mean after change point: ${parameters[\"mu2_mean\"]:.2f}')\n",
    "        \n",
    "        return parameters\n",
    "    \n",
    "    def calculate_impact(self, change_point_results, parameters):\n",
    "        \"\"\"\n",
    "        Quantify the impact of the detected change point.\n",
    "        \"\"\"\n",
    "        self.logger.info('Calculating impact of change point')\n",
    "        \n",
    "        mu1 = parameters['mu1_mean']\n",
    "        mu2 = parameters['mu2_mean']\n",
    "        \n",
    "        absolute_change = mu2 - mu1\n",
    "        percent_change = (absolute_change / mu1) * 100 if mu1 != 0 else 0\n",
    "        \n",
    "        impact = {\n",
    "            'absolute_change': absolute_change,\n",
    "            'percent_change': percent_change,\n",
    "            'direction': 'Increase' if absolute_change > 0 else 'Decrease',\n",
    "            'magnitude': 'Large' if abs(percent_change) > 50 else 'Moderate' if abs(percent_change) > 20 else 'Small'\n",
    "        }\n",
    "        \n",
    "        return impact\n",
    "    \n",
    "    def display_results(self, cp_results, param_results, impact):\n",
    "        \"\"\"\n",
    "        Display comprehensive results.\n",
    "        \"\"\"\n",
    "        print('\\n' + '='*80)\n",
    "        print('BAYESIAN CHANGE POINT ANALYSIS RESULTS')\n",
    "        print('='*80)\n",
    "        \n",
    "        print(f\"\\nCHANGE POINT DETECTION:\")\n",
    "        print('-'*60)\n",
    "        print(f\"  Detected Date: {cp_results['cp_date'].date()}\")\n",
    "        print(f\"  Time Index: {int(cp_results['tau_median'])} days from start\")\n",
    "        print(f\"  95% Credible Interval: Days {int(cp_results['tau_hdi'][0])} to {int(cp_results['tau_hdi'][1])}\")\n",
    "        print(f\"  Certainty: 95% CI spans {int(cp_results['tau_hdi'][1] - cp_results['tau_hdi'][0])} days\")\n",
    "        \n",
    "        print(f\"\\nPRICE PARAMETERS:\")\n",
    "        print('-'*60)\n",
    "        print(f\"  Mean Price BEFORE Change Point: ${param_results['mu1_mean']:.2f}/barrel\")\n",
    "        print(f\"  95% CI: ${param_results['mu1_hdi'][0]:.2f} - ${param_results['mu1_hdi'][1]:.2f}\")\n",
    "        print(f\"\\n  Mean Price AFTER Change Point: ${param_results['mu2_mean']:.2f}/barrel\")\n",
    "        print(f\"  95% CI: ${param_results['mu2_hdi'][0]:.2f} - ${param_results['mu2_hdi'][1]:.2f}\")\n",
    "        print(f\"\\n  Volatility (Sigma): ${param_results['sigma_mean']:.2f}/barrel\")\n",
    "        \n",
    "        print(f\"\\nIMPACT QUANTIFICATION:\")\n",
    "        print('-'*60)\n",
    "        print(f\"  Absolute Change: ${impact['absolute_change']:.2f}/barrel\")\n",
    "        print(f\"  Percentage Change: {impact['percent_change']:.2f}%\")\n",
    "        print(f\"  Direction: {impact['direction']}\")\n",
    "        print(f\"  Magnitude: {impact['magnitude']}\")\n",
    "        \n",
    "        print('\\n' + '='*80)\n",
    "\n",
    "# Extract and interpret results\n",
    "interpreter = ChangePointInterpreter(cp_model.model, cp_model.trace, df)\n",
    "cp_results = interpreter.extract_change_point()\n",
    "param_results = interpreter.extract_parameters()\n",
    "impact = interpreter.calculate_impact(cp_results, param_results)\n",
    "interpreter.display_results(cp_results, param_results, impact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior Distribution Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 6: Posterior Distribution Visualizations\n",
    "class PosteriorVisualizer:\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations of posterior distributions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, trace, data, cp_results, param_results):\n",
    "        self.trace = trace\n",
    "        self.data = data\n",
    "        self.cp_results = cp_results\n",
    "        self.param_results = param_results\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def plot_change_point_posterior(self):\n",
    "        \"\"\"\n",
    "        Plot posterior distribution of change point.\n",
    "        \"\"\"\n",
    "        self.logger.info('Plotting change point posterior')\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "        \n",
    "        tau_samples = self.cp_results['tau_samples']\n",
    "        \n",
    "        # Histogram of tau samples\n",
    "        axes[0].hist(tau_samples, bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "        axes[0].axvline(self.cp_results['tau_median'], color='red', linestyle='--', linewidth=2, label='Median')\n",
    "        axes[0].axvline(self.cp_results['tau_hdi'][0], color='orange', linestyle=':', linewidth=2, label='95% HDI')\n",
    "        axes[0].axvline(self.cp_results['tau_hdi'][1], color='orange', linestyle=':', linewidth=2)\n",
    "        axes[0].set_title('Posterior Distribution of Change Point', fontsize=12, fontweight='bold')\n",
    "        axes[0].set_xlabel('Time Index (Days from Start)')\n",
    "        axes[0].set_ylabel('Frequency')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Price series with change point overlay\n",
    "        axes[1].plot(self.data['Date'], self.data['Price'], linewidth=1, alpha=0.6, label='Observed Price')\n",
    "        cp_date = self.cp_results['cp_date']\n",
    "        axes[1].axvline(cp_date, color='red', linestyle='--', linewidth=2, label=f'Change Point ({cp_date.date()})')\n",
    "        axes[1].fill_betweenx(\n",
    "            [self.data['Price'].min(), self.data['Price'].max()],\n",
    "            self.data['Date'].iloc[int(self.cp_results['tau_hdi'][0])],\n",
    "            self.data['Date'].iloc[int(self.cp_results['tau_hdi'][1])],\n",
    "            alpha=0.2, color='orange', label='95% Credible Interval'\n",
    "        )\n",
    "        axes[1].set_title('Change Point in Time Series Context', fontsize=12, fontweight='bold')\n",
    "        axes[1].set_ylabel('Price (USD/barrel)')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('task2_04_change_point_posterior.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        self.logger.info('Change point posterior plot saved')\n",
    "    \n",
    "    def plot_parameter_posteriors(self):\n",
    "        \"\"\"\n",
    "        Plot posterior distributions of parameters.\n",
    "        \"\"\"\n",
    "        self.logger.info('Plotting parameter posteriors')\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # mu1\n",
    "        axes[0, 0].hist(self.param_results['mu1_samples'], bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "        axes[0, 0].axvline(self.param_results['mu1_mean'], color='red', linestyle='--', linewidth=2)\n",
    "        axes[0, 0].set_title('Posterior: Mean Price BEFORE Change Point', fontsize=11, fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Price (USD/barrel)')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # mu2\n",
    "        axes[0, 1].hist(self.param_results['mu2_samples'], bins=50, color='darkgreen', alpha=0.7, edgecolor='black')\n",
    "        axes[0, 1].axvline(self.param_results['mu2_mean'], color='red', linestyle='--', linewidth=2)\n",
    "        axes[0, 1].set_title('Posterior: Mean Price AFTER Change Point', fontsize=11, fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Price (USD/barrel)')\n",
    "        axes[0, 1].set_ylabel('Frequency')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Sigma\n",
    "        axes[1, 0].hist(self.param_results['sigma_samples'], bins=50, color='darkred', alpha=0.7, edgecolor='black')\n",
    "        axes[1, 0].axvline(self.param_results['sigma_mean'], color='red', linestyle='--', linewidth=2)\n",
    "        axes[1, 0].set_title('Posterior: Volatility (Sigma)', fontsize=11, fontweight='bold')\n",
    "        axes[1, 0].set_xlabel('Volatility (USD/barrel)')\n",
    "        axes[1, 0].set_ylabel('Frequency')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Comparison of means\n",
    "        axes[1, 1].hist(self.param_results['mu1_samples'], bins=40, alpha=0.5, label='Before', color='steelblue')\n",
    "        axes[1, 1].hist(self.param_results['mu2_samples'], bins=40, alpha=0.5, label='After', color='darkgreen')\n",
    "        axes[1, 1].set_title('Mean Price Comparison', fontsize=11, fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('Price (USD/barrel)')\n",
    "        axes[1, 1].set_ylabel('Frequency')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('task2_05_parameter_posteriors.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        self.logger.info('Parameter posterior plots saved')\n",
    "\n",
    "# Create visualizations\n",
    "viz = PosteriorVisualizer(cp_model.trace, df, cp_results, param_results)\n",
    "viz.plot_change_point_posterior()\n",
    "viz.plot_parameter_posteriors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Event Association and Causal Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 7: Associate Change Points with Historical Events\n",
    "class EventAssociator:\n",
    "    \"\"\"\n",
    "    Associate detected change points with historical events.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, events_file='major_events.csv'):\n",
    "        self.data = data\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        try:\n",
    "            self.events_df = pd.read_csv(events_file)\n",
    "            self.events_df['date'] = pd.to_datetime(self.events_df['date'])\n",
    "            self.logger.info(f'Loaded {len(self.events_df)} events from {events_file}')\n",
    "        except FileNotFoundError:\n",
    "            self.logger.warning(f'Events file {events_file} not found, creating sample events')\n",
    "            self.events_df = pd.DataFrame()\n",
    "    \n",
    "    def find_nearby_events(self, cp_date, window_days=60):\n",
    "        \"\"\"\n",
    "        Find events that occurred near the detected change point.\n",
    "        \"\"\"\n",
    "        self.logger.info(f'Finding events within {window_days} days of {cp_date.date()}')\n",
    "        \n",
    "        if len(self.events_df) == 0:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        time_diff = (self.events_df['date'] - cp_date).dt.total_seconds() / (24 * 3600)\n",
    "        nearby_mask = (time_diff.abs() <= window_days) & (time_diff >= -30)  # Event before CP is more relevant\n",
    "        nearby_events = self.events_df[nearby_mask].copy()\n",
    "        nearby_events['days_before_cp'] = -time_diff[nearby_mask].values\n",
    "        \n",
    "        return nearby_events.sort_values('days_before_cp')\n",
    "    \n",
    "    def create_event_impact_narrative(self, cp_date, cp_results, param_results, impact):\n",
    "        \"\"\"\n",
    "        Create narrative explaining the change point and associated events.\n",
    "        \"\"\"\n",
    "        nearby_events = self.find_nearby_events(cp_date, window_days=90)\n",
    "        \n",
    "        narrative = {\n",
    "            'change_point_date': cp_date,\n",
    "            'detected_date': cp_results['cp_date'],\n",
    "            'confidence_interval': (cp_results['tau_hdi'][0], cp_results['tau_hdi'][1]),\n",
    "            'price_before': param_results['mu1_mean'],\n",
    "            'price_after': param_results['mu2_mean'],\n",
    "            'absolute_change': impact['absolute_change'],\n",
    "            'percent_change': impact['percent_change'],\n",
    "            'nearby_events': nearby_events\n",
    "        }\n",
    "        \n",
    "        return narrative\n",
    "    \n",
    "    def display_event_associations(self, narrative):\n",
    "        \"\"\"\n",
    "        Display event associations in readable format.\n",
    "        \"\"\"\n",
    "        print('\\n' + '='*80)\n",
    "        print('EVENT ASSOCIATION AND CAUSAL INTERPRETATION')\n",
    "        print('='*80)\n",
    "        \n",
    "        print(f\"\\nDetected Change Point: {narrative['detected_date'].date()}\")\n",
    "        print(f\"95% Credible Interval: {narrative['detected_date'].date()}\")\n",
    "        print(f\"\\nPrice Shift Summary:\")\n",
    "        print(f\"  Before: ${narrative['price_before']:.2f}/barrel\")\n",
    "        print(f\"  After: ${narrative['price_after']:.2f}/barrel\")\n",
    "        print(f\"  Change: ${narrative['absolute_change']:.2f} ({narrative['percent_change']:.2f}%)\")\n",
    "        \n",
    "        if len(narrative['nearby_events']) > 0:\n",
    "            print(f\"\\nNearby Events (within 90 days):\")\n",
    "            print('-'*80)\n",
    "            for idx, event in narrative['nearby_events'].iterrows():\n",
    "                print(f\"\\n  {event['event_name']} ({event['date'].date()})\")\n",
    "                print(f\"    Category: {event['category']}\")\n",
    "                print(f\"    Description: {event['description']}\")\n",
    "                print(f\"    Days before change point: {event.get('days_before_cp', 'N/A')}\")\n",
    "        else:\n",
    "            print(f\"\\n  No events found in major events dataset within 90-day window\")\n",
    "            print(f\"  This may indicate:\")\n",
    "            print(f\"    1. Change point driven by gradual market forces\")\n",
    "            print(f\"    2. Small-scale events not captured in major events list\")\n",
    "            print(f\"    3. Anticipated market reaction ahead of formal event\")\n",
    "        \n",
    "        print('\\nIMPORTANT: Association does not imply causation!')\n",
    "        print('Temporal correlation may reflect:\")\n",
    "        print('  • Market anticipation of known events')\n",
    "        print('  • Delayed market reaction to earlier shocks')\n",
    "        print('  • Coincidental timing with unrelated factors')\n",
    "        print('='*80)\n",
    "\n",
    "# Perform event association\n",
    "associator = EventAssociator(df, 'major_events.csv')\n",
    "narrative = associator.create_event_impact_narrative(\n",
    "    cp_results['cp_date'],\n",
    "    cp_results,\n",
    "    param_results,\n",
    "    impact\n",
    ")\n",
    "associator.display_event_associations(narrative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 8: Task 2 Summary\n",
    "print('\\n' + '='*80)\n",
    "print('TASK 2 COMPLETION SUMMARY')\n",
    "print('='*80)\n",
    "\n",
    "print(f'''\n✓ DELIVERABLES COMPLETED:\n\n1. Data Preparation & EDA\n   ✓ Data loaded and validated: {len(df)} daily observations\n   ✓ Log returns calculated and analyzed\n   ✓ Rolling statistics computed\n   ✓ Visualizations created:\n     - Price series with trends\n     - Daily returns distribution  \n     - Volatility patterns\n\n2. Bayesian Change Point Model\n   ✓ PyMC model structure defined\n   ✓ Priors specified for tau, mu1, mu2, sigma\n   ✓ MCMC sampling completed\n   ✓ Convergence diagnostics checked (r_hat values)\n   ✓ Trace plots generated\n\n3. Model Results\n   ✓ Change point detected: {cp_results['cp_date'].date()}\n   ✓ 95% Credible Interval: {int(cp_results['tau_hdi'][0])} - {int(cp_results['tau_hdi'][1])} days\n   ✓ Mean before change point: ${param_results['mu1_mean']:.2f}/barrel\n   ✓ Mean after change point: ${param_results['mu2_mean']:.2f}/barrel\n   ✓ Absolute change: ${impact['absolute_change']:.2f}/barrel ({impact['percent_change']:.2f}%)\n\n4. Event Association\n   ✓ Nearby events identified and analyzed\n   ✓ Hypotheses formulated about causal links\n   ✓ Important caveats about correlation vs causation\n\n5. Visualizations Generated\n   - task2_01_eda_visualizations.png\n   - task2_02_rolling_statistics.png\n   - task2_03_trace_plots.png\n   - task2_04_change_point_posterior.png\n   - task2_05_parameter_posteriors.png\n\n6. Analysis Artifacts\n   - Full model trace with posterior samples\n   - Comprehensive model summary\n   - Event association results\n   - Quantified impact metrics\n\nNEXT STEPS:\n→ Task 3: Build Interactive Dashboard\n→ Flask backend for API endpoints\n→ React frontend with interactive visualizations\n→ Final reporting and communication\n\nKEY INSIGHTS:\n• Bayesian approach provides probabilistic certainty about change point timing\n• Multiple visualizations show convergence and posterior distributions\n• Event association suggests potential causal mechanisms\n• Model limitations properly documented (see assumptions from Task 1)\n\nIMPORTANT NOTES:\n1. This is a single change point model; data may have multiple regimes\n2. Temporal association ≠ causation (requires additional validation)\n3. Model assumes constant variance; extensions could allow regime-specific volatility\n4. Results should be interpreted probabilistically, not deterministically\n''')\n\nprint('='*80)\nlogger.info('Task 2: Change Point Modeling - COMPLETED')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
