{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Laying the Foundation for Analysis\n",
    "## Change Point Analysis and Statistical Modeling of Brent Oil Prices\n",
    "\n",
    "**Objective**: Define the data analysis workflow and develop a thorough understanding of the model and data.\n",
    "\n",
    "**Due Date**: Interim Submission - Sunday, 08 Feb 2026, 8:00 PM UTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 1: Import Libraries and Configure Logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "        logging.FileHandler('task_1_analysis.log')\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info('Task 1: Laying the Foundation - Analysis Started')\n",
    "logger.info(f'Timestamp: {datetime.now()}')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Define the Data Analysis Workflow\n",
    "\n",
    "### Step 1.1: Document Analysis Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 2: Define Analysis Workflow\n",
    "class DataAnalysisWorkflow:\n",
    "    \"\"\"\n",
    "    Modularized workflow for Brent oil price analysis.\n",
    "    Follows data science best practices with structured steps.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.workflow_steps = []\n",
    "    \n",
    "    def document_workflow(self):\n",
    "        \"\"\"\n",
    "        Define and document the complete analysis pipeline.\n",
    "        \"\"\"\n",
    "        self.workflow_steps = [\n",
    "            {\n",
    "                'step': 1,\n",
    "                'phase': 'Data Loading & Preparation',\n",
    "                'tasks': [\n",
    "                    'Load Brent oil price CSV',\n",
    "                    'Convert date column to datetime format',\n",
    "                    'Handle missing values',\n",
    "                    'Sort data chronologically',\n",
    "                    'Validate data quality'\n",
    "                ],\n",
    "                'dependencies': None,\n",
    "                'tools': ['pandas', 'numpy']\n",
    "            },\n",
    "            {\n",
    "                'step': 2,\n",
    "                'phase': 'Exploratory Data Analysis (EDA)',\n",
    "                'tasks': [\n",
    "                    'Analyze time series properties (trend, seasonality)',\n",
    "                    'Test stationarity (ADF test)',\n",
    "                    'Calculate log returns',\n",
    "                    'Analyze volatility patterns',\n",
    "                    'Visualize price movements and shocks'\n",
    "                ],\n",
    "                'dependencies': [1],\n",
    "                'tools': ['statsmodels', 'matplotlib', 'seaborn']\n",
    "            },\n",
    "            {\n",
    "                'step': 3,\n",
    "                'phase': 'Research & Event Compilation',\n",
    "                'tasks': [\n",
    "                    'Research major geopolitical events',\n",
    "                    'Research OPEC policy decisions',\n",
    "                    'Research economic/sanctions events',\n",
    "                    'Compile structured event dataset',\n",
    "                    'Create event CSV with dates and descriptions'\n",
    "                ],\n",
    "                'dependencies': None,\n",
    "                'tools': ['csv', 'pandas']\n",
    "            },\n",
    "            {\n",
    "                'step': 4,\n",
    "                'phase': 'Model Understanding',\n",
    "                'tasks': [\n",
    "                    'Study Bayesian change point theory',\n",
    "                    'Review PyMC documentation',\n",
    "                    'Understand switch points and priors',\n",
    "                    'Plan model architecture',\n",
    "                    'Document assumptions and limitations'\n",
    "                ],\n",
    "                'dependencies': [2],\n",
    "                'tools': ['PyMC', 'documentation']\n",
    "            },\n",
    "            {\n",
    "                'step': 5,\n",
    "                'phase': 'Change Point Modeling',\n",
    "                'tasks': [\n",
    "                    'Build Bayesian change point model in PyMC',\n",
    "                    'Define priors and likelihood',\n",
    "                    'Run MCMC sampling',\n",
    "                    'Check convergence diagnostics',\n",
    "                    'Extract and interpret posterior distributions'\n",
    "                ],\n",
    "                'dependencies': [1, 4],\n",
    "                'tools': ['PyMC', 'arviz']\n",
    "            },\n",
    "            {\n",
    "                'step': 6,\n",
    "                'phase': 'Event Association & Interpretation',\n",
    "                'tasks': [\n",
    "                    'Map detected change points to dates',\n",
    "                    'Match change points with historical events',\n",
    "                    'Quantify price impact per event',\n",
    "                    'Calculate percentage changes',\n",
    "                    'Formulate hypotheses about causation'\n",
    "                ],\n",
    "                'dependencies': [3, 5],\n",
    "                'tools': ['pandas', 'numpy', 'visualization']\n",
    "            },\n",
    "            {\n",
    "                'step': 7,\n",
    "                'phase': 'Dashboard Development',\n",
    "                'tasks': [\n",
    "                    'Design Flask API endpoints',\n",
    "                    'Create React frontend components',\n",
    "                    'Build interactive visualizations',\n",
    "                    'Implement event highlighting',\n",
    "                    'Add date range filtering'\n",
    "                ],\n",
    "                'dependencies': [5, 6],\n",
    "                'tools': ['Flask', 'React', 'Recharts']\n",
    "            },\n",
    "            {\n",
    "                'step': 8,\n",
    "                'phase': 'Reporting & Communication',\n",
    "                'tasks': [\n",
    "                    'Create comprehensive report',\n",
    "                    'Generate summary statistics',\n",
    "                    'Produce final visualizations',\n",
    "                    'Write executive summary',\n",
    "                    'Document limitations and future work'\n",
    "                ],\n",
    "                'dependencies': [5, 6, 7],\n",
    "                'tools': ['pandas', 'matplotlib', 'documentation']\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        self.logger.info(f'Workflow documented with {len(self.workflow_steps)} major phases')\n",
    "        return self.workflow_steps\n",
    "    \n",
    "    def display_workflow(self):\n",
    "        \"\"\"\n",
    "        Display the workflow in a readable format.\n",
    "        \"\"\"\n",
    "        print('\\n' + '='*80)\n",
    "        print('DATA ANALYSIS WORKFLOW - BRENT OIL PRICES')\n",
    "        print('='*80 + '\\n')\n",
    "        \n",
    "        for step in self.workflow_steps:\n",
    "            print(f\"\\nStep {step['step']}: {step['phase'].upper()}\")\n",
    "            print('-' * 60)\n",
    "            for task in step['tasks']:\n",
    "                print(f\"  • {task}\")\n",
    "            print(f\"  Tools: {', '.join(step['tools'])}\")\n",
    "            if step['dependencies']:\n",
    "                print(f\"  Dependencies: Steps {step['dependencies']}\")\n",
    "        print('\\n' + '='*80)\n",
    "\n",
    "# Execute workflow documentation\n",
    "workflow = DataAnalysisWorkflow()\n",
    "workflow.document_workflow()\n",
    "workflow.display_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2: Load and Validate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 3: Data Loading Module\n",
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    Modular class for loading and validating Brent oil price data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.df = None\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load CSV data and perform initial validation.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f'Loading data from {self.filepath}')\n",
    "            self.df = pd.read_csv(self.filepath)\n",
    "            self.logger.info(f'Data loaded successfully. Shape: {self.df.shape}')\n",
    "            return self.df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f'Error loading data: {str(e)}')\n",
    "            raise\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Convert date column and handle missing values.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Convert date column\n",
    "            self.logger.info('Converting Date column to datetime format')\n",
    "            self.df['Date'] = pd.to_datetime(self.df['Date'], format='%d-%b-%y')\n",
    "            \n",
    "            # Sort by date\n",
    "            self.logger.info('Sorting data by date')\n",
    "            self.df = self.df.sort_values('Date').reset_index(drop=True)\n",
    "            \n",
    "            # Convert price to numeric\n",
    "            self.df['Price'] = pd.to_numeric(self.df['Price'], errors='coerce')\n",
    "            \n",
    "            # Check for missing values\n",
    "            missing_count = self.df.isnull().sum().sum()\n",
    "            if missing_count > 0:\n",
    "                self.logger.warning(f'Found {missing_count} missing values')\n",
    "                self.df = self.df.dropna()\n",
    "                self.logger.info(f'After removing missing values: {self.df.shape}')\n",
    "            else:\n",
    "                self.logger.info('No missing values found')\n",
    "            \n",
    "            return self.df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f'Error preprocessing data: {str(e)}')\n",
    "            raise\n",
    "    \n",
    "    def validate_data(self):\n",
    "        \"\"\"\n",
    "        Validate data quality and consistency.\n",
    "        \"\"\"\n",
    "        self.logger.info('Validating data quality')\n",
    "        \n",
    "        validation_results = {\n",
    "            'total_records': len(self.df),\n",
    "            'date_range': f\"{self.df['Date'].min().date()} to {self.df['Date'].max().date()}\",\n",
    "            'price_range': f\"${self.df['Price'].min():.2f} - ${self.df['Price'].max():.2f}\",\n",
    "            'null_values': self.df.isnull().sum().to_dict(),\n",
    "            'data_types': self.df.dtypes.to_dict()\n",
    "        }\n",
    "        \n",
    "        print('\\nDATA VALIDATION RESULTS')\n",
    "        print('=' * 50)\n",
    "        for key, value in validation_results.items():\n",
    "            print(f'{key}: {value}')\n",
    "        \n",
    "        self.logger.info('Data validation completed successfully')\n",
    "        return validation_results\n",
    "\n",
    "# Execute data loading\n",
    "data_loader = DataLoader('BrentOilPrices.csv')\n",
    "df = data_loader.load_data()\n",
    "df = data_loader.preprocess_data()\n",
    "validation = data_loader.validate_data()\n",
    "\n",
    "print('\\nFirst few records:')\n",
    "print(df.head(10))\n",
    "print('\\nLast few records:')\n",
    "print(df.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.3: Research and Compile Major Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 4: Event Compilation Module\n",
    "class EventCompiler:\n",
    "    \"\"\"\n",
    "    Module for compiling major geopolitical and economic events\n",
    "    that may have impacted Brent oil prices.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.events = []\n",
    "    \n",
    "    def compile_events(self):\n",
    "        \"\"\"\n",
    "        Compile a comprehensive list of major events from 1987-2022.\n",
    "        Focus on: Geopolitical, OPEC decisions, sanctions, and economic shocks.\n",
    "        \"\"\"\n",
    "        self.events = [\n",
    "            {\n",
    "                'date': '1990-08-02',\n",
    "                'event_name': 'Iraq Invasion of Kuwait',\n",
    "                'category': 'Geopolitical Conflict',\n",
    "                'description': 'Iraqi invasion triggers major oil supply concerns and price spike'\n",
    "            },\n",
    "            {\n",
    "                'date': '1991-01-17',\n",
    "                'event_name': 'Gulf War Begins',\n",
    "                'category': 'Armed Conflict',\n",
    "                'description': 'Operation Desert Storm commences, oil markets volatile'\n",
    "            },\n",
    "            {\n",
    "                'date': '1997-07-02',\n",
    "                'event_name': 'Asian Financial Crisis',\n",
    "                'category': 'Economic Crisis',\n",
    "                'description': 'Currency collapse spreads across Asia, demand shock for commodities'\n",
    "            },\n",
    "            {\n",
    "                'date': '2001-09-11',\n",
    "                'event_name': 'September 11 Attacks',\n",
    "                'category': 'Terrorism/Geopolitical',\n",
    "                'description': 'Terrorist attacks in US impact global markets and economic outlook'\n",
    "            },\n",
    "            {\n",
    "                'date': '2003-03-20',\n",
    "                'event_name': 'Iraq War Begins',\n",
    "                'category': 'Armed Conflict',\n",
    "                'description': 'US-led invasion of Iraq, supply disruptions expected'\n",
    "            },\n",
    "            {\n",
    "                'date': '2004-01-01',\n",
    "                'event_name': 'Oil Prices Begin Sustained Rise',\n",
    "                'category': 'Market Trend',\n",
    "                'description': 'Start of the 2004-2008 oil boom, driven by supply constraints and demand'\n",
    "            },\n",
    "            {\n",
    "                'date': '2008-07-11',\n",
    "                'event_name': 'Oil Prices Peak',\n",
    "                'category': 'Price Peak',\n",
    "                'description': 'Brent crude reaches all-time high of ~$145/barrel'\n",
    "            },\n",
    "            {\n",
    "                'date': '2008-09-15',\n",
    "                'event_name': 'Lehman Brothers Collapse',\n",
    "                'category': 'Financial Crisis',\n",
    "                'description': 'Global financial crisis triggers demand collapse and oil price crash'\n",
    "            },\n",
    "            {\n",
    "                'date': '2011-03-15',\n",
    "                'event_name': 'Libyan Civil War',\n",
    "                'category': 'Geopolitical Conflict',\n",
    "                'description': 'Uprising in Libya disrupts major oil production, prices spike'\n",
    "            },\n",
    "            {\n",
    "                'date': '2014-06-01',\n",
    "                'event_name': 'Oil Price Decline Begins',\n",
    "                'category': 'Market Trend',\n",
    "                'description': 'Saudi Arabia increases production, leading to sustained price decline'\n",
    "            },\n",
    "            {\n",
    "                'date': '2014-11-27',\n",
    "                'event_name': 'OPEC Abandons Production Cuts',\n",
    "                'category': 'OPEC Policy',\n",
    "                'description': 'OPEC decision to maintain production accelerates oil price collapse'\n",
    "            },\n",
    "            {\n",
    "                'date': '2016-02-11',\n",
    "                'event_name': 'Oil Prices Hit Bottom',\n",
    "                'category': 'Price Low',\n",
    "                'description': 'Brent crude falls to ~$26/barrel during global supply glut'\n",
    "            },\n",
    "            {\n",
    "                'date': '2016-11-30',\n",
    "                'event_name': 'OPEC Announces Production Cuts',\n",
    "                'category': 'OPEC Policy',\n",
    "                'description': 'OPEC agrees to reduce production to support prices'\n",
    "            },\n",
    "            {\n",
    "                'date': '2020-02-01',\n",
    "                'event_name': 'COVID-19 Pandemic Begins',\n",
    "                'category': 'Health Crisis',\n",
    "                'description': 'Global pandemic triggers economic shutdown and oil demand collapse'\n",
    "            },\n",
    "            {\n",
    "                'date': '2020-04-20',\n",
    "                'event_name': 'Oil Prices Turn Negative',\n",
    "                'category': 'Price Anomaly',\n",
    "                'description': 'May crude futures turn negative for first time in history'\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        self.logger.info(f'Compiled {len(self.events)} major events')\n",
    "        return self.events\n",
    "    \n",
    "    def export_to_csv(self, filepath='major_events.csv'):\n",
    "        \"\"\"\n",
    "        Export compiled events to CSV for reference.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            events_df = pd.DataFrame(self.events)\n",
    "            events_df.to_csv(filepath, index=False)\n",
    "            self.logger.info(f'Events exported to {filepath}')\n",
    "            return events_df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f'Error exporting events: {str(e)}')\n",
    "            raise\n",
    "    \n",
    "    def display_events(self):\n",
    "        \"\"\"\n",
    "        Display events in a formatted table.\n",
    "        \"\"\"\n",
    "        events_df = pd.DataFrame(self.events)\n",
    "        print('\\nMAJOR EVENTS IMPACTING BRENT OIL PRICES (1987-2022)')\n",
    "        print('=' * 100)\n",
    "        print(events_df.to_string(index=False))\n",
    "        print('=' * 100)\n",
    "        return events_df\n",
    "\n",
    "# Execute event compilation\n",
    "event_compiler = EventCompiler()\n",
    "events = event_compiler.compile_events()\n",
    "events_df = event_compiler.display_events()\n",
    "events_df = event_compiler.export_to_csv()\n",
    "\n",
    "# Summary statistics\n",
    "print('\\nEVENT CATEGORY SUMMARY')\n",
    "print(events_df['category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.4: Document Assumptions and Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 5: Assumptions and Limitations Documentation\n",
    "class AssumptionsAndLimitations:\n",
    "    \"\"\"\n",
    "    Document key assumptions and limitations of the analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def document(self):\n",
    "        doc = {\n",
    "            'assumptions': [\n",
    "                {\n",
    "                    'area': 'Data Quality',\n",
    "                    'assumption': 'Brent oil price data is accurate and complete',\n",
    "                    'rationale': 'Data sourced from established financial databases',\n",
    "                    'risk': 'Low - industry standard data source'\n",
    "                },\n",
    "                {\n",
    "                    'area': 'Model Specification',\n",
    "                    'assumption': 'A single change point exists in the mean price',\n",
    "                    'rationale': 'Simplified model for interpretability; extended models can test multiple change points',\n",
    "                    'risk': 'Medium - real data may have multiple regime shifts'\n",
    "                },\n",
    "                {\n",
    "                    'area': 'Statistical Independence',\n",
    "                    'assumption': 'Daily price changes are conditionally independent given the regime',\n",
    "                    'rationale': 'Simplifying assumption for tractable inference',\n",
    "                    'risk': 'Medium - oil prices show autocorrelation and clustering'\n",
    "                },\n",
    "                {\n",
    "                    'area': 'Normality',\n",
    "                    'assumption': 'Log returns approximately follow a normal distribution',\n",
    "                    'rationale': 'Standard assumption in financial modeling',\n",
    "                    'risk': 'Medium - market data often exhibits heavy tails'\n",
    "                },\n",
    "                {\n",
    "                    'area': 'Event Timing',\n",
    "                    'assumption': 'Events occur on documented dates with immediate market impact',\n",
    "                    'rationale': 'Market-efficient hypothesis assumption',\n",
    "                    'risk': 'High - market reactions may lag or anticipate events'\n",
    "                },\n",
    "                {\n",
    "                    'area': 'Causal Attribution',\n",
    "                    'assumption': 'Detected change points can be attributed to identified events',\n",
    "                    'rationale': 'For hypothesis generation and interpretation',\n",
    "                    'risk': 'High - correlation does not imply causation'\n",
    "                }\n",
    "            ],\n",
    "            'limitations': [\n",
    "                {\n",
    "                    'category': 'Temporal Scope',\n",
    "                    'limitation': 'Analysis covers 1987-2022; pre-1987 dynamics may differ',\n",
    "                    'impact': 'Results apply primarily to modern energy markets'\n",
    "                },\n",
    "                {\n",
    "                    'category': 'Univariate Analysis',\n",
    "                    'limitation': 'Only examines Brent oil prices; ignores other commodities and macroeconomic variables',\n",
    "                    'impact': 'Cannot capture spillovers or multivariate relationships'\n",
    "                },\n",
    "                {\n",
    "                    'category': 'Event Data Quality',\n",
    "                    'limitation': 'Event dates are approximate; exact market impact timing is uncertain',\n",
    "                    'impact': 'Change point may lead or lag reported event date'\n",
    "                },\n",
    "                {\n",
    "                    'category': 'Model Simplicity',\n",
    "                    'limitation': 'Bayesian change point model assumes constant variance within regimes',\n",
    "                    'impact': 'Cannot capture regime-specific volatility changes'\n",
    "                },\n",
    "                {\n",
    "                    'category': 'Inference Uncertainty',\n",
    "                    'limitation': 'MCMC sampling introduces uncertainty in posterior estimates',\n",
    "                    'impact': 'Results should be interpreted probabilistically, not deterministically'\n",
    "                },\n",
    "                {\n",
    "                    'category': 'Confounding Factors',\n",
    "                    'limitation': 'Cannot isolate individual event effects when multiple events occur simultaneously',\n",
    "                    'impact': 'Attribution to specific causes becomes ambiguous'\n",
    "                }\n",
    "            ],\n",
    "            'correlation_vs_causation': {\n",
    "                'key_distinction': 'A detected change point coinciding with an event does NOT prove causation',\n",
    "                'correlation': 'Temporal association between change point and event',\n",
    "                'causation': 'Event directly caused the price regime shift',\n",
    "                'requirements_for_causation': [\n",
    "                    'Temporal precedence (event must occur before effect)',\n",
    "                    'Covariation (change point timing matches event)',\n",
    "                    'No plausible alternative explanations',\n",
    "                    'Mechanism (clear economic rationale)',\n",
    "                    'Dose-response relationship (larger shocks produce larger effects)'\n",
    "                ],\n",
    "                'approach': 'This analysis identifies correlations and formulates hypotheses; causation requires additional evidence (e.g., IV models, natural experiments, expert validation)'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return doc\n",
    "    \n",
    "    def export_to_file(self, doc, filepath='assumptions_and_limitations.txt'):\n",
    "        \"\"\"\n",
    "        Export detailed documentation to file.\n",
    "        \"\"\"\n",
    "        with open(filepath, 'w') as f:\n",
    "            f.write('ASSUMPTIONS AND LIMITATIONS DOCUMENTATION\\n')\n",
    "            f.write('='*80 + '\\n\\n')\n",
    "            \n",
    "            f.write('KEY ASSUMPTIONS\\n')\n",
    "            f.write('-'*80 + '\\n')\n",
    "            for i, assumption in enumerate(doc['assumptions'], 1):\n",
    "                f.write(f\"\\n{i}. {assumption['area']}\\n\")\n",
    "                f.write(f\"   Assumption: {assumption['assumption']}\\n\")\n",
    "                f.write(f\"   Rationale: {assumption['rationale']}\\n\")\n",
    "                f.write(f\"   Risk Level: {assumption['risk']}\\n\")\n",
    "            \n",
    "            f.write('\\n\\nKEY LIMITATIONS\\n')\n",
    "            f.write('-'*80 + '\\n')\n",
    "            for i, limitation in enumerate(doc['limitations'], 1):\n",
    "                f.write(f\"\\n{i}. {limitation['category']}\\n\")\n",
    "                f.write(f\"   Limitation: {limitation['limitation']}\\n\")\n",
    "                f.write(f\"   Impact: {limitation['impact']}\\n\")\n",
    "            \n",
    "            f.write('\\n\\nCORRELATION VS CAUSATION\\n')\n",
    "            f.write('-'*80 + '\\n')\n",
    "            cv = doc['correlation_vs_causation']\n",
    "            f.write(f\"\\nKey Distinction: {cv['key_distinction']}\\n\\n\")\n",
    "            f.write(f\"Correlation: {cv['correlation']}\\n\")\n",
    "            f.write(f\"Causation: {cv['causation']}\\n\\n\")\n",
    "            f.write(\"Requirements for Establishing Causation:\\n\")\n",
    "            for j, req in enumerate(cv['requirements_for_causation'], 1):\n",
    "                f.write(f\"  {j}. {req}\\n\")\n",
    "            f.write(f\"\\nApproach: {cv['approach']}\\n\")\n",
    "        \n",
    "        self.logger.info(f'Assumptions and limitations exported to {filepath}')\n",
    "\n",
    "# Execute documentation\n",
    "doc_module = AssumptionsAndLimitations()\n",
    "doc_content = doc_module.document()\n",
    "doc_module.export_to_file(doc_content)\n",
    "\n",
    "# Display summary\n",
    "print('\\nKEY ASSUMPTIONS SUMMARY')\n",
    "print('='*80)\n",
    "for assumption in doc_content['assumptions']:\n",
    "    print(f\"\\n{assumption['area']}:\")\n",
    "    print(f\"  • {assumption['assumption']}\")\n",
    "    print(f\"  • Risk: {assumption['risk']}\")\n",
    "\n",
    "print('\\n\\nCORRELATION VS CAUSATION - KEY POINT')\n",
    "print('='*80)\n",
    "print(doc_content['correlation_vs_causation']['key_distinction'])\n",
    "print(f\"\\nThis analysis: {doc_content['correlation_vs_causation']['approach']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Time Series Properties Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 6: Time Series Properties Analysis\n",
    "class TimeSeriesAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyze key properties of the Brent oil price time series.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def calculate_log_returns(self):\n",
    "        \"\"\"\n",
    "        Calculate log returns for stationarity analysis.\n",
    "        \"\"\"\n",
    "        self.logger.info('Calculating log returns')\n",
    "        self.data['Log_Returns'] = np.log(self.data['Price'] / self.data['Price'].shift(1))\n",
    "        self.data['Log_Returns'] = self.data['Log_Returns'].fillna(0)\n",
    "        return self.data\n",
    "    \n",
    "    def plot_price_series(self):\n",
    "        \"\"\"\n",
    "        Visualize raw price series over time.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(16, 6))\n",
    "        plt.plot(self.data['Date'], self.data['Price'], linewidth=1.5, color='darkblue')\n",
    "        plt.title('Brent Crude Oil Prices (1987-2022)', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('Price (USD per barrel)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('01_brent_price_series.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        self.logger.info('Price series plot saved')\n",
    "    \n",
    "    def plot_log_returns(self):\n",
    "        \"\"\"\n",
    "        Visualize log returns and volatility clustering.\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "        \n",
    "        # Log returns\n",
    "        axes[0].plot(self.data['Date'], self.data['Log_Returns'], linewidth=0.5, color='darkgreen')\n",
    "        axes[0].set_title('Daily Log Returns of Brent Oil Prices', fontsize=12, fontweight='bold')\n",
    "        axes[0].set_ylabel('Log Return')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Rolling volatility\n",
    "        rolling_vol = self.data['Log_Returns'].rolling(window=30).std()\n",
    "        axes[1].plot(self.data['Date'], rolling_vol, linewidth=1, color='darkred')\n",
    "        axes[1].set_title('30-Day Rolling Volatility', fontsize=12, fontweight='bold')\n",
    "        axes[1].set_ylabel('Standard Deviation')\n",
    "        axes[1].set_xlabel('Year')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('02_log_returns_volatility.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        self.logger.info('Log returns and volatility plot saved')\n",
    "    \n",
    "    def summary_statistics(self):\n",
    "        \"\"\"\n",
    "        Calculate descriptive statistics.\n",
    "        \"\"\"\n",
    "        self.logger.info('Calculating summary statistics')\n",
    "        \n",
    "        stats = {\n",
    "            'Price': {\n",
    "                'Mean': self.data['Price'].mean(),\n",
    "                'Median': self.data['Price'].median(),\n",
    "                'Std Dev': self.data['Price'].std(),\n",
    "                'Min': self.data['Price'].min(),\n",
    "                'Max': self.data['Price'].max(),\n",
    "                'Range': self.data['Price'].max() - self.data['Price'].min(),\n",
    "                'CV': (self.data['Price'].std() / self.data['Price'].mean()) * 100\n",
    "            },\n",
    "            'Log_Returns': {\n",
    "                'Mean': self.data['Log_Returns'].mean(),\n",
    "                'Median': self.data['Log_Returns'].median(),\n",
    "                'Std Dev': self.data['Log_Returns'].std(),\n",
    "                'Min': self.data['Log_Returns'].min(),\n",
    "                'Max': self.data['Log_Returns'].max(),\n",
    "                'Skewness': self.data['Log_Returns'].skew(),\n",
    "                'Kurtosis': self.data['Log_Returns'].kurtosis()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print('\\nTIME SERIES SUMMARY STATISTICS')\n",
    "        print('='*60)\n",
    "        print('\\nPRICE STATISTICS:')\n",
    "        for key, value in stats['Price'].items():\n",
    "            if key == 'CV':\n",
    "                print(f'  {key}: {value:.2f}%')\n",
    "            else:\n",
    "                print(f'  {key}: {value:.4f}')\n",
    "        \n",
    "        print('\\nLOG RETURNS STATISTICS:')\n",
    "        for key, value in stats['Log_Returns'].items():\n",
    "            print(f'  {key}: {value:.6f}')\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# Execute time series analysis\n",
    "ts_analyzer = TimeSeriesAnalyzer(df.copy())\n",
    "df = ts_analyzer.calculate_log_returns()\n",
    "ts_analyzer.plot_price_series()\n",
    "ts_analyzer.plot_log_returns()\n",
    "stats = ts_analyzer.summary_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.5: Stationarity Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 7: Stationarity Testing\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "\n",
    "class StationarityTester:\n",
    "    \"\"\"\n",
    "    Test for stationarity using ADF and KPSS tests.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def adf_test(self, series, name):\n",
    "        \"\"\"\n",
    "        Perform Augmented Dickey-Fuller test.\n",
    "        Null hypothesis: unit root (non-stationary)\n",
    "        \"\"\"\n",
    "        self.logger.info(f'Running ADF test on {name}')\n",
    "        result = adfuller(series.dropna(), autolag='AIC')\n",
    "        \n",
    "        return {\n",
    "            'name': name,\n",
    "            'test': 'ADF',\n",
    "            'test_statistic': result[0],\n",
    "            'p_value': result[1],\n",
    "            'critical_values': result[4],\n",
    "            'stationary': result[1] < 0.05\n",
    "        }\n",
    "    \n",
    "    def kpss_test(self, series, name):\n",
    "        \"\"\"\n",
    "        Perform KPSS test.\n",
    "        Null hypothesis: stationarity\n",
    "        \"\"\"\n",
    "        self.logger.info(f'Running KPSS test on {name}')\n",
    "        result = kpss(series.dropna(), regression='c')\n",
    "        \n",
    "        return {\n",
    "            'name': name,\n",
    "            'test': 'KPSS',\n",
    "            'test_statistic': result[0],\n",
    "            'p_value': result[1],\n",
    "            'critical_values': result[3],\n",
    "            'stationary': result[1] > 0.05\n",
    "        }\n",
    "    \n",
    "    def perform_tests(self):\n",
    "        \"\"\"\n",
    "        Perform all stationarity tests.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Test on price series\n",
    "        results.append(self.adf_test(self.data['Price'], 'Price Series'))\n",
    "        results.append(self.kpss_test(self.data['Price'], 'Price Series'))\n",
    "        \n",
    "        # Test on log returns\n",
    "        results.append(self.adf_test(self.data['Log_Returns'], 'Log Returns'))\n",
    "        results.append(self.kpss_test(self.data['Log_Returns'], 'Log Returns'))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def display_results(self, results):\n",
    "        \"\"\"\n",
    "        Display test results in readable format.\n",
    "        \"\"\"\n",
    "        print('\\nSTATIONARITY TEST RESULTS')\n",
    "        print('='*80)\n",
    "        \n",
    "        for result in results:\n",
    "            print(f\"\\n{result['name']} - {result['test']} Test\")\n",
    "            print('-'*60)\n",
    "            print(f\"  Test Statistic: {result['test_statistic']:.6f}\")\n",
    "            print(f\"  P-value: {result['p_value']:.6f}\")\n",
    "            print(f\"  Stationary (α=0.05): {result['stationary']}\")\n",
    "            \n",
    "            if result['test'] == 'ADF':\n",
    "                print(f\"  Interpretation: Reject H0 (non-stationary) at 5% level\" if result['stationary'] \n",
    "                      else f\"  Interpretation: Fail to reject H0 - likely non-stationary\")\n",
    "            else:\n",
    "                print(f\"  Interpretation: Fail to reject H0 (stationary) at 5% level\" if result['stationary']\n",
    "                      else f\"  Interpretation: Reject H0 - likely non-stationary\")\n",
    "        \n",
    "        print('\\n' + '='*80)\n",
    "        print('\\nSUMMARY FOR MODELING:')\n",
    "        print('  • Price Series: Non-stationary (as expected for prices))')\n",
    "        print('  • Log Returns: Stationary (suitable for modeling)')\n",
    "        print('  → Recommend using log returns or differencing in change point model')\n\n",
    "# Execute stationarity tests\n",
    "stationarity_tester = StationarityTester(df)\n",
    "test_results = stationarity_tester.perform_tests()\n",
    "stationarity_tester.display_results(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 8: Task 1 Summary and Deliverables Checklist\n",
    "print('\\n' + '='*80)\n",
    "print('TASK 1 COMPLETION SUMMARY')\n",
    "print('='*80)\n",
    "\n",
    "print(f'''\n✓ DELIVERABLES COMPLETED:\n\n1. Data Analysis Workflow\n   - 8-phase comprehensive workflow documented\n   - All steps, tools, and dependencies defined\n   - Ready for implementation\n\n2. Data Preparation & Validation\n   - Data loaded: {len(df)} records\n   - Date range: {df['Date'].min().date()} to {df['Date'].max().date()}\n   - Price range: ${df['Price'].min():.2f} - ${df['Price'].max():.2f}\n   - Missing values: Handled\n   - Data quality: Validated ✓\n\n3. Major Events Compilation\n   - 15 key geopolitical and economic events compiled\n   - Categories: Conflicts, OPEC decisions, financial crises\n   - Exported to: major_events.csv\n\n4. Assumptions & Limitations Documentation\n   - 6 core assumptions documented with risk levels\n   - 6 limitations identified with impact assessment\n   - Correlation vs Causation framework explained\n   - File: assumptions_and_limitations.txt\n\n5. Time Series Analysis\n   - Summary statistics calculated\n   - Stationarity tests completed\n   - Visualizations generated:\n     * Price series plot\n     * Log returns and volatility plots\n\n6. Ready for Next Phase:\n   - Data prepared and validated\n   - Event data compiled and structured\n   - Assumptions documented\n   - Properties understood for modeling\n\nNEXT STEPS:\n→ Move to Task 2: Bayesian Change Point Modeling\n→ Build PyMC model with identified properties\n→ Run MCMC sampling and analyze results\n\nFILES GENERATED:\n- major_events.csv\n- assumptions_and_limitations.txt\n- 01_brent_price_series.png\n- 02_log_returns_volatility.png\n- task_1_analysis.log\n''')\n\nprint('='*80)\nlogger.info('Task 1: Foundation Analysis - COMPLETED')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
